{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pylab\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pregnancies  Insulin   BMI  Age  Glucose  BloodPressure  \\\n",
      "0            6        0  33.6   50      148             72   \n",
      "1            1        0  26.6   31       85             66   \n",
      "2            8        0  23.3   32      183             64   \n",
      "3            1       94  28.1   21       89             66   \n",
      "4            0      168  43.1   33      137             40   \n",
      "\n",
      "   DiabetesPedigreeFunction  \n",
      "0                     0.627  \n",
      "1                     0.351  \n",
      "2                     0.672  \n",
      "3                     0.167  \n",
      "4                     2.288  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"diabetes.csv\")\n",
    "\n",
    "feature_cols = ['Pregnancies', 'Insulin', 'BMI', 'Age','Glucose','BloodPressure','DiabetesPedigreeFunction']\n",
    "x = data[feature_cols] # Features\n",
    "y = data.Outcome # Target variable\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n",
    "print(x.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[114  32]\n",
      " [ 46  39]]\n",
      "Accuracy 0.6623376623376623\n",
      "Auc Score Prob 0.6198227236099919\n",
      "Recall 0.4588235294117647\n",
      "Precission 0.5492957746478874\n",
      "F1 Score 0.5\n",
      "                          Importance\n",
      "Glucose                     0.319714\n",
      "BMI                         0.213853\n",
      "DiabetesPedigreeFunction    0.131080\n",
      "BloodPressure               0.128553\n",
      "Age                         0.113079\n",
      "Insulin                     0.048055\n",
      "Pregnancies                 0.045666\n"
     ]
    }
   ],
   "source": [
    "# Create Decision Tree Classifier\n",
    "clsfr = DecisionTreeClassifier()\n",
    "clsfr = clsfr.fit(x_train, y_train)\n",
    "\n",
    "pred = clsfr.predict(x_test)\n",
    "pred_prob = clsfr.predict_proba(x_test)[:,1]\n",
    "\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(\"Accuracy\", accuracy_score(y_test, pred))\n",
    "print(\"Auc Score Prob\", roc_auc_score(y_test, pred_prob))\n",
    "print(\"Recall\", recall_score(y_test, pred))\n",
    "print(\"Precission\", precision_score(y_test, pred))\n",
    "print(\"F1 Score\", f1_score(y_test, pred))\n",
    "\n",
    "feature_imp = pd.DataFrame(clsfr.feature_importances_, index=x_train.columns, columns=[\"Importance\"]).sort_values(\n",
    "                                \"Importance\", ascending=False)\n",
    "print(feature_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[124  22]\n",
      " [ 31  54]]\n",
      "Accuracy 0.7705627705627706\n",
      "Auc Score Prob 0.84709911361805\n",
      "Recall 0.6352941176470588\n",
      "Precission 0.7105263157894737\n",
      "F1 Score 0.6708074534161491\n",
      "                          Importance\n",
      "Glucose                     0.562837\n",
      "BMI                         0.326594\n",
      "Age                         0.110569\n",
      "Pregnancies                 0.000000\n",
      "Insulin                     0.000000\n",
      "BloodPressure               0.000000\n",
      "DiabetesPedigreeFunction    0.000000\n"
     ]
    }
   ],
   "source": [
    "clsfr2 = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "clsfr2 = clsfr2.fit(x_train, y_train)\n",
    "\n",
    "pred2 = clsfr2.predict(x_test)\n",
    "pred2_prob = clsfr2.predict_proba(x_test)[:,1]\n",
    "\n",
    "print(confusion_matrix(y_test, pred2))\n",
    "print(\"Accuracy\", accuracy_score(y_test, pred2))\n",
    "print(\"Auc Score Prob\", roc_auc_score(y_test, pred2_prob))\n",
    "print(\"Recall\", recall_score(y_test, pred2))\n",
    "print(\"Precission\", precision_score(y_test, pred2))\n",
    "print(\"F1 Score\", f1_score(y_test, pred2))\n",
    "\n",
    "feature_imp2 = pd.DataFrame(clsfr2.feature_importances_, index=x_train.columns, columns=[\"Importance\"]).sort_values(\n",
    "                                \"Importance\", ascending=False)\n",
    "print(feature_imp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[127  19]\n",
      " [ 30  55]]\n",
      "Accuracy 0.7878787878787878\n",
      "Auc Score Prob 0.8624093473005641\n",
      "Recall 0.6470588235294118\n",
      "Precission 0.7432432432432432\n",
      "F1 Score 0.6918238993710691\n",
      "[0.07159728 0.05451045 0.20221884 0.13038158 0.30864438 0.09496335\n",
      " 0.13768413]\n"
     ]
    }
   ],
   "source": [
    "# Bagging\n",
    "bag = BaggingClassifier(base_estimator=clsfr, n_estimators=100, random_state=42)\n",
    "bag = bag.fit(x_train, y_train)\n",
    "\n",
    "pred3 = bag.predict(x_test)\n",
    "pred3_prob = bag.predict_proba(x_test)[:,1]\n",
    "\n",
    "print(confusion_matrix(y_test, pred3))\n",
    "print(\"Accuracy\", accuracy_score(y_test, pred3))\n",
    "print(\"Auc Score Prob\", roc_auc_score(y_test, pred3_prob))\n",
    "print(\"Recall\", recall_score(y_test, pred3))\n",
    "print(\"Precission\", precision_score(y_test, pred3))\n",
    "print(\"F1 Score\", f1_score(y_test, pred3))\n",
    "\n",
    "\n",
    "featimp = np.mean([\n",
    "    tree.feature_importances_ for tree in bag.estimators_\n",
    "], axis=0)\n",
    "\n",
    "feature_importances = pd.DataFrame(featimp,\n",
    "                                   index = x_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(featimp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[131  15]\n",
      " [ 36  49]]\n",
      "Accuracy 0.7792207792207793\n",
      "Auc Score Prob 0.8764705882352941\n",
      "Recall 0.5764705882352941\n",
      "Precission 0.765625\n",
      "F1 Score 0.6577181208053692\n",
      "[0.02651369 0.00973136 0.26448318 0.10728876 0.52629193 0.01647792\n",
      " 0.04921316]\n"
     ]
    }
   ],
   "source": [
    "# Bagging2\n",
    "bag = BaggingClassifier(base_estimator=clsfr2, n_estimators=100, random_state=42)\n",
    "bag = bag.fit(x_train, y_train)\n",
    "\n",
    "pred4 = bag.predict(x_test)\n",
    "pred4_prob = bag.predict_proba(x_test)[:,1]\n",
    "\n",
    "print(confusion_matrix(y_test, pred4))\n",
    "print(\"Accuracy\", accuracy_score(y_test, pred4))\n",
    "print(\"Auc Score Prob\", roc_auc_score(y_test, pred4_prob))\n",
    "print(\"Recall\", recall_score(y_test, pred4))\n",
    "print(\"Precission\", precision_score(y_test, pred4))\n",
    "print(\"F1 Score\", f1_score(y_test, pred4))\n",
    "\n",
    "\n",
    "featimport = np.mean([\n",
    "    tree.feature_importances_ for tree in bag.estimators_\n",
    "], axis=0)\n",
    "\n",
    "feature_importances = pd.DataFrame(featimport,\n",
    "                                   index = x_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(featimport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[132  14]\n",
      " [ 32  53]]\n",
      "Accuracy: 0.8008658008658008\n",
      "AUC Score: 0.7638195004029009\n",
      "AUC Score prob: 0.8596696212731667\n",
      "Precision: 0.7910447761194029\n",
      "Recall: 0.6235294117647059\n",
      "F1 Score: 0.6973684210526316\n",
      "                          importance\n",
      "Glucose                     0.287053\n",
      "BMI                         0.191048\n",
      "Age                         0.145789\n",
      "DiabetesPedigreeFunction    0.144158\n",
      "BloodPressure               0.092014\n",
      "Pregnancies                 0.079983\n",
      "Insulin                     0.059956\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, max_features=4)\n",
    "\n",
    "rfc = rfc.fit(x_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = rfc.predict(x_test)\n",
    "y_pred_prob = rfc.predict_proba(x_test)[:,1]\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print (\"AUC Score:\", roc_auc_score(y_test, y_pred))\n",
    "print (\"AUC Score prob:\", roc_auc_score(y_test, y_pred_prob))\n",
    "print (\"Precision:\", precision_score(y_test, y_pred))\n",
    "print (\"Recall:\", recall_score(y_test, y_pred))\n",
    "print (\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "\n",
    "feature_importances = pd.DataFrame(rfc.feature_importances_,\n",
    "                                   index = x_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[128  18]\n",
      " [ 30  55]]\n",
      "Accuracy: 0.7922077922077922\n",
      "AUC Score: 0.7618855761482675\n",
      "AUC Score prob: 0.8287671232876712\n",
      "Precision: 0.7534246575342466\n",
      "Recall: 0.6470588235294118\n",
      "F1 Score: 0.6962025316455697\n",
      "                          importance\n",
      "Glucose                     0.287053\n",
      "BMI                         0.191048\n",
      "Age                         0.145789\n",
      "DiabetesPedigreeFunction    0.144158\n",
      "BloodPressure               0.092014\n",
      "Pregnancies                 0.079983\n",
      "Insulin                     0.059956\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "abc = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "abc = abc.fit(x_train, y_train)\n",
    "\n",
    "pred = abc.predict(x_test)\n",
    "pred_prob = abc.predict_proba(x_test)[:,1]\n",
    "\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
    "print (\"AUC Score:\", roc_auc_score(y_test, pred))\n",
    "print (\"AUC Score prob:\", roc_auc_score(y_test, pred_prob))\n",
    "print (\"Precision:\", precision_score(y_test, pred))\n",
    "print (\"Recall:\", recall_score(y_test, pred))\n",
    "print (\"F1 Score:\", f1_score(y_test, pred))\n",
    "\n",
    "feature_importances = pd.DataFrame(rfc.feature_importances_,\n",
    "                                   index = x_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting will not perform well enough if the Data is small and the number of rows is low"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
