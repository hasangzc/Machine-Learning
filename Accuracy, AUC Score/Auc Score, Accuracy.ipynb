{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.7083333333333334\n",
      "AUC Score prob: 0.875\n",
      "Accuracy:  0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "test_y = [1, 0, 0, 1, 0, 1, 1]\n",
    "\n",
    "y_prediction = [1, 0, 1, 1, 0, 1, 0]\n",
    "y_pred_probability = [.6, .4, .5, .9, .2, .7, .4]\n",
    "\n",
    "# Note //  The correct calculation for AUC Score is on probability!\n",
    "\n",
    "print(\"AUC Score:\", roc_auc_score(test_y, y_prediction))\n",
    "print(\"AUC Score prob:\", roc_auc_score(test_y, y_pred_probability))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_y, y_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.75\n",
      "AUC Score prob: 0.9166666666666667\n",
      "Accuracy:  0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "y_prediction2 = [0, 0, 0, 1, 0, 1, 0]\n",
    "y_pred_probability2 = [.4, .3, .4, .8, .2, .6, .4]\n",
    "\n",
    "print(\"AUC Score:\", roc_auc_score(test_y, y_prediction2))\n",
    "print(\"AUC Score prob:\", roc_auc_score(test_y, y_pred_probability2))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_y, y_prediction2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.875\n",
      "AUC Score prob: 0.75\n",
      "Accuracy:  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "y_prediction3 = [1, 0, 0, 1, 0, 1, 0]\n",
    "y_pred_probability3 = [.51, .49, .49, .51, .49, .51, .4]\n",
    "\n",
    "print(\"AUC Score:\", roc_auc_score(test_y, y_prediction3))\n",
    "print(\"AUC Score prob:\", roc_auc_score(test_y, y_pred_probability3))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_y, y_prediction3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.8333333333333334\n",
      "AUC Score prob: 0.9166666666666667\n",
      "Accuracy:  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "y_prediction4 = [1, 0, 1, 1, 0, 1, 1]\n",
    "y_pred_probability4 = [.4, .3, .4, .7, .2, .6, .4]\n",
    "\n",
    "print(\"AUC Score:\", roc_auc_score(test_y, y_prediction4))\n",
    "print(\"AUC Score prob:\", roc_auc_score(test_y, y_pred_probability4))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_y, y_prediction4))\n",
    "\n",
    "# Decision boundary has changed but as you can see AUC Score is not a good metric for this!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
